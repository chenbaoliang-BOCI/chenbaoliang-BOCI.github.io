---
title: 由HashMap和HashTable说开
date: 2018-03-14 22:55:16
tags: [Java]
---

# HashMap 和 HashTable 的区别

- 父类不同，不过这点不重要。
- HashMap 的键、值可为 null，HashTable 都不行。
- HashMap 非线程安全，HashTable 通过在所有闭包方法上加上 synchronized 关键词来实现线程安全。
- 基于上一点，HashMap 的速度要明显更快。

除了这些区别外，还有一些细节的但是重要的区别。

两者都是立足在哈希的基础上实现快速查找。两者的哈希方法有所不同。

**HashMap**

```java
static final int hash(Object key) {
    int h;
    // java8 之前多做了几次混淆
    // h ^= (h >>> 20) ^ (h >>> 12);
    // return h ^ (h >>> 7) ^ (h >>> 4);
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

**HashTable**

```java
int hash = key.hashCode();
int index = (hash & 0x7FFFFFFF) % tab.length;
```

HashTable 做了一次模运算，略慢一点。

HashMap 混淆了高低位的信息之后对低位进行掩码，而 HashTable 的散列程度直接取决于整体的哈希值的散列程度。也就是说，要实现相同的散列程度，HashTable 对于 key 对象本身的  hashCode() 方法要求更高。

# 对象的 hashCode() 方法

## hashCode() 的抽象定义

hashCode() 方法是散列算法的一个实现。散列算法的本质是在两个集合之间建立映射关系。之所以可以通过散列实现快速搜索定位，是因为：

1. 原象集的空间往往是非常大，甚至是无穷的，但实际应用中可以通过某种途径（比如经验、推算）认为只会用到原象集的一个有限子集，这样将原象集映射到一个子集，缩小搜索空间。

2. 有时候因为当前技术的限制，无法快速对原象集中的元素进行信息抽取，因此寻找一个方便抽取信息的象集，并在两者之间建立映射。

以上两点典型的应用就是 Java 中对象跟整数之间的映射。

最理想的散列算法是在原象集和象集之间建立满射，这样实际上只用到了上述第 2 点，但这种理想情况很少（至少我还没注意到过）。

次理想的算法有两种：满射非单射、单射非满射。前者是用时间换空间，后者是用空间换时间。这两种设计方式并不互斥，Java 中的 HashMap 和 HashTable 就是这种设计（当然可能还有其他类）。

Java 根类 `Object` 中对 hashCode() 方法的设计有详细描述。关键点有：

1. 此方法建立的是非满射非单射关系——业务上相同的对象应该映射到相同的整数，业务上不同的对象有可能映射到相同的整数。此外还有一个隐含的但是比较显而易见的点——并没有强制要求完全利用整个 Java 整数空间，因为业务对象的枚举完全可能没那么多。
2. 对于没有重写此方法的类，用的是 Java 默认的逻辑——将对象的内存地址映射成一个整数。也就是说业务上相同的两个对象会映射到不同的整数。

## 常用的散列算法

### 除法散列算法

> index = value % divisor

### 平方散列算法

> index = (value * value) >> num

## String 的 hashCode()

```java
// s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1]
public int hashCode() {
        int h = hash;
        if (h == 0 && value.length > 0) {
            char val[] = value;

            for (int i = 0; i < value.length; i++) {
                h = 31 * h + val[i];
            }
            hash = h;
        }
        return h;
    }
```

容易理解的一点就是每个字符都参与了运算，因此最后结果包含了每个字符的信息。

这里的乘数 31 是个关键点。之所以选 31 有几个明显的原因：

1. 会被 Java 底层优化成 (h << 5) - h。
2. 质数作为乘数减小乘积冲突的概率。（最后散列值是否冲突很大程度上取决于散列算法，而乘积是否冲突只是一个前置因素，很难简单地推测乘积冲突对于散列结果的影响，因此这一点原因不是很重要，基本上只是出于习惯）
3. 31 大小适中：若乘数较大，则更多字符串的乘积会大于整数上限，截取低 31 位相当于在一个大集合中选中一个真子集，因而导致信息丢失，继而增大冲突概率；若乘数较小，则更多字符串的乘积挤在整数空间里，不利于均匀分摊。

关于上述第 3 点，这里只是定性描述。实际上选取 31 是经过了对一批数字进行筛选评估之后的结果。

**从 String 的例子可以总结几点设计 hashCode() 方法的要点：**

- 运算简单；
- 每一个影响两个对象是否相同的成员都要参与散列运算；
- 综合评估对象所处集合的规模，既要充分利用整数空间，又要尽量避免数位溢出导致信息丢失。

## Effective Java 中的指导性建议

1. 把某个非零的常数值保存在一个名为 result 的 int 型变量中。
2. 对于对象中每个关键域 f（指 equals 方法中涉及的每个域），完成以下步骤：
   1. 为该域计算 int 型的散列码 c：
      1. 如果该域是 boolean 类型，则计算 `(f ? 1 : 0)`。
      2. 如果该域是 byte、char、short 或者 int 类型，则计算 `(int)f`。
      3. 如果该域是 long 类型，则计算 `(int)(f ^ (f >>> 32))`。
      4. 如果该域是 float 类型，则计算 `Float.floatToIntBits(f)`。
      5. 如果该域是 double 类型，则计算 `Double.doubleToLongBits(f)`，然后按照步骤 2.1.3，为 long 类型计算散列值。
      6. 如果该域是一个对象引用，并且该类的 equals 方法通过递归地调用 equals 的方式来比较这个域，则同样为这个域递归地调用 hashCode。如果需要更复杂的比较，则为这个域计算一个“范式（canonical representation）”，然后针对这个范式调用 hashCode。如果这个域的值为 null，则返回一个常数（通常是 0）。
      7. 如果该域是一个数组，则要把每一个元素当作单独的域来处理。也就是说，递归地应用上述规则，对每个重要的元素计算一个散列码，然后根据 2.2 中的做法把这些散列值组合起来。如果数组域中的每个元素都很重要，可以利用 Java 1.5 新增的 `Arrays.hashCode` 方法。
    2. 把 2.1 中计算得到的散列码 c 合并到 result 中 `result = 31 * result + c`。
    3. 检查“相等的实例是否都具有相等的散列码”。

## 关于设计 hashCode() 方法的一点补充

优秀的散列算法不光要完成两个集合的映射，还要尽量在映射的过程中均摊每个元素的出现概率，也就是调整每个元素提供的信息量。

拿英文字母举个例子。ASCII 码中，对应的两个大小写字母之间只有第 6 位比特存在区别（比如A=0100 0001，a=0110 0001）。在实际应用中，小写字母出现的概率要明显大于大写字母。因此第 6 位比特携带的信息量就较小，需要在映射过程中从其他比特均摊一部分信息量过来。

除此之外，如果只考虑大写（或小写）字母，每个字母出现的概率也是不同的。总之基本思路就是要将原象集中概率不同的元素尽量映射成象集中概率接近的元素。

# 散列冲突的解决办法

绝大多数情况下，散列空间中的元素冲突只是概率大小的问题，无法完全避免。解决冲突的办法主要有几种：

- 线性再散列法
- 非线性再散列法
- 外部拉链法

## 线性再散列法（开放定址法）

主体思路就是当遇到冲突时，按照某个移动步长在集合中搜索下一个位置，一直到找到空位。

根据所定的步长的不同，又分为

- 线性探查法：步长固定为 1。
- 线性补偿探查法：步长为 Q，要求 Q 与集合长度 m 互质（为了最终能遍历所有元素）。
- 随机探查法：步长随机。

3 种方法半斤八两，没有本质区别，都会导致以下问题：

- 在散列程度差的元素附近形成堆聚。
- 劣币驱逐良币：散列程度差的元素挤占了原本属于散列程度好的元素的位置，导致后者不得不去挤占别人的位置，进而也变成了“坏元素”。
- 删除元素的时候不能物理删除，只能逻辑删除，因为后面进来的元素的位置取决于前面“占坑”的元素。若物理删除了前面某个“占坑”的元素，则显然没法正确找到后面想找的元素。
- 随着冲突概率的增大、空间占用的增多，在集合中遍历所花费的时间迅速增长。

## 非线性再散列法

遇到冲突时就对当前散列值再做一次散列，直到找到空位。其实就是用非线性的运算计算步长。跟线性再散列法没有本质区别。除了拥有线性再散列法的缺点外，还隐性地把性能依赖于散列算法本身的性能。

## 外部拉链法

比较常用的办法，也是 HashMap 和 HashTable 使用的办法。

每个元素保存的不是单个业务对象，而是一个由若干个具有相同散列值的业务对象组成的列表（一般都是用单链表，方便频繁的增删操作）。

外部拉链法的缺点是需要更多的空间。不过现在内存空间很充裕，反而时间成本越来越高，所以绝大部分情况下都算不上问题。

另外，我注意到 HashMap 和 HashTable 在插入新元素的时候都是插到链表头部。可是在实际插入新元素之前肯定是已经遍历了整个链表，确定没有 equals 的对象，这时已经拿到了尾端节点的引用，只要做一次引用赋值就能插到链表尾端，为什么还要在链表头部做两次引用赋值？初步猜测是因为设计者觉得多一次引用赋值的开销可以忽略，而新插入的元素往往具有更高的访问概率，插在头部减小链表遍历的期望值。
