---
title: 字符集和编码
date: 2017-01-01 09:00:00
tags: [字符集和编码]
---

# 一些主要编码的区别

摘自 [developerWorks-深入分析 Java 中的中文编码问题](https://www.ibm.com/developerworks/cn/java/j-lo-chinesecoding/)

<!-- more -->

- ASCII 码
总共有 128 个，用一个字节的低 7 位表示，0~31 是控制字符如换行回车删除等；32~126 是打印字符，可以通过键盘输入并且能够显示出来。
- ISO-8859-1
128 个字符显然是不够用的，于是 ISO 组织在 ASCII 码基础上又制定了一些列标准用来扩展 ASCII 编码，它们是 ISO-8859-1~ISO-8859-15，其中 ISO-8859-1 涵盖了大多数西欧语言字符，所有应用的最广泛。ISO-8859-1 仍然是单字节编码，它总共能表示 256 个字符。
- GB2312
它的全称是《信息交换用汉字编码字符集 基本集》，它是双字节编码，总的编码范围是 A1-F7，其中从 A1-A9 是符号区，总共包含 682 个符号，从 B0-F7 是汉字区，包含 6763 个汉字。
- GBK
全称叫《汉字内码扩展规范》，是国家技术监督局为 windows95 所制定的新的汉字内码规范，它的出现是为了扩展 GB2312，加入更多的汉字，它的编码范围是 8140~FEFE（去掉 XX7F）总共有 23940 个码位，它能表示 21003 个汉字，它的编码是和 GB2312 兼容的，也就是说用 GB2312 编码的汉字可以用 GBK 来解码，并且不会有乱码。
- GB18030
全称是《信息交换用汉字编码字符集》，是我国的强制标准，它可能是单字节、双字节或者四字节编码，它的编码与 GB2312 编码兼容，这个虽然是国家标准，但是实际应用系统中使用的并不广泛。
- UTF-16
说到 UTF 必须要提到 Unicode（Universal Code 统一码），ISO 试图想创建一个全新的超语言字典，世界上所有的语言都可以通过这本字典来相互翻译。可想而知这个字典是多么的复杂，关于 Unicode 的详细规范可以参考相应文档。Unicode 是 Java 和 XML 的基础，下面详细介绍 Unicode 在计算机中的存储形式。
UTF-16 具体定义了 Unicode 字符在计算机中存取方法。UTF-16 用两个字节来表示 Unicode 转化格式，这个是定长的表示方法，不论什么字符都可以用两个字节表示，两个字节是 16 个 bit，所以叫 UTF-16。UTF-16 表示字符非常方便，每两个字节表示一个字符，这个在字符串操作时就大大简化了操作，这也是 Java 以 UTF-16 作为内存的字符存储格式的一个很重要的原因。

---
# Unicode 历史

<table>
    <tr>
        <td>国际标准化组织（ISO）</td>
        <td>统一码联盟</td>
    </tr>
    <tr>
        <td>ISO/IEC 10646 项目，UCS（Universal Character Set, UCS）字符集</td>
        <td>Unicode 项目，Unicode 字符集</td>
    </tr>
    <tr>
        <td colspan="2" style="text-align:center;">1991 年前后开始合作</td>
    </tr>
    <tr>
        <td colspan='2' style="text-align:center;">从 Unicode 2.0 开始，两者字库和字码基本相同</td>
    </tr>
</table>

---
# UCS

> UCS-2（Universal Character Set coded in 2 octets）用两个字节表示码点。
后因需要表示更多字符，提出 UCS-4，用四个字节表示码点。

- UCS-2:U+0000 ~ U+FFFF，UCS-4:U+00000000 ~ U+7FFFFFFF。从 Unicode 2.0 开始，ISO 承诺不会替超出 U+10FFFF 的码点赋值，以与统一码联盟的项目保持一致。
- UCS-4 第一字节最高位固定为 0，因而根据第一字节可分为 128 个 group。
- 根据第二字节分为 256 个 plane。
- 根据第三字节分为 256 个 row。
- 根据第四字节分为 256 个 cell。
- group 0 中的 plane 0 称为 BMP（Basic Multilingual Plane）。BMP 中的码点与 UCS-2 相同。
- UCS-2 和 UCS-4 的 BMP 中的 0xD800 ~ 0xDFFF 称为代理区（Surrogate），其中的码点不对应任何字符，仅用来区分某两个字节应解析为 UCS-2 字符还是与相邻的两个字节一起解析为 UCS-4 字符。
- 中文范围 4E00-9FA5：CJK 统一表意符号 (CJK Unified Ideographs)

---
# UTF

> UTF（Unicode Transformation Format） 是对 Unicode 字符集的编码方式，有多种。

## UTF-8

|Unicode/UCS-4     |UTF-8                                       |备注|
|:-----------------|:-------------------------------------------|:---|
|0000~007F         |0XXXXXXX                                    |    |
|0080~07FF         |110XXXXX 10XXXXXX                           |    |
|0800~FFFF         |1110XXXX 10XXXXXX 10XXXXXX                  |基本定义范围：0~FFFF|
|1 0000~1F FFFF    |11110XXX 10XXXXXX 10XXXXXX 10XXXXXX         |Unicode 6.1定义范围：0~10 FFFF|
|20 0000~3FF FFFF  |111110XX 10XXXXXX 10XXXXXX 10XXXXXX 10XXXXXX|说明：此非 Unicode 编码范围，属于 UCS-4 编码。早期的规范 UTF-8 可以到达 6 字节序列，可以覆盖到 31 位元（通用字符集原来的极限）。尽管如此，2003年11月 UTF-8 被 RFC 3629 重新规范，只能使用原来 Unicode 定义的区域， U+0000 到 U+10FFFF。根据规范，这些字节值将无法出现在合法 UTF-8 序列中。|
|400 0000~7FFF FFFF|1111110X 10XXXXXX 10XXXXXX 10XXXXXX 10XXXXXX 10XXXXXX|同上|

1. 确定待编码字符所在的编码范围。
2. 设对应的模板有 n 个“x”。将字符的编码写成二进制，不足 n 位则在高位补 0。
3. 将二进制数写入“x”对应的位置。

## UTF-16

|Unicode         |UTF-16           |备注|
|:---------------|:----------------|:---|
|0x0000~0xFFFF   |xxxxxxxx xxxxxxxx|此范围内 UFT-16 的编码与 UCS 的编码相同，其中 0xD800~0xDFFF 不对应任何字符|
|0x10000~0x10FFFF|110110xx xxxxxxxx 110111xx xxxxxxxx|将码点减去 0x10000 后高位补 0 至 20 位，依次对应 20 个“x”|

为了将双字节的编码与四字节的编码区分开，设计者将 0xD800 ~ 0xDFFF 保留下来作为代理区（Surrogate）。由于 BMP 中的对应范围不映射任何字符，故不会产生混淆。

|码点范围     |名称                       |名称        |
|:------------|:--------------------------|:-----------|
|0xD800~0xDB7F|High Surrogates            |高位替代    |
|0xDB80~0xDBFF|High Private Use Surrogates|高位专用替代|
|0xDC00~0xDFFF|Low Surrogates             |低位替代    |

高、低为替代分别表示四字节的两个高字节和两个低字节。0xDB80 ~ 0xDBFF 之所以称为高位专用替代，是因为该范围对应的码点范围是平面 15 和平面 16，这两个平面被用作专用区。

## UTF-32

UCS-4 编码本身就是 UTF-32 的编码。


## 字节序

- “大端”（Big Endian, BE）和“小端”（Little Endian, LE）。
-  Unicode 固定以字节为单位进行编码，所以需要确定字节序而不是比特序。
-  LE 即先读写低字节。
-  UTF-8 的编码方式使得它本身可以不带 BOM，而 Unicode 的标准是 UTF-8 可带可不带 BOM。一些程序和平台为了兼容以前的程序，默认使用带 BOM 的 UTF-8。

Unicode 标准建议用 BOM（Byte Order Mark）来区分字节序，即在传输字节流前，先传输被作为 BOM 的字符“零宽无中断空格”，这些字符在 Unicode 中都是未定义的码位，不应该出现在实际传输中。

|UTF 编码         |BOM        |
|:----------------|:----------|
|UTF-8 without BOM|无         |
|UTF-8 with BOM   |EF BB BF   |
|UTF-16LE         |FF FE      |
|UTF-16BE         |FE FF      |
|UTF-32LE         |FF FE 00 00|
|UTF-32BE         |00 00 FE FF|


---
# ANSI 和 ASCII

ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）是基于拉丁字母的一套电脑编码系统，使用一个字节表示一个字符。

ANSI（American National Standards Institute）从某种角度来说是 ASCII 的超集。它通过为各个国家/语言的编码分别创建一个代码页来使用同一套内码表示不同的字符。由于同一个内码对应了不同代码页上的多个字符，故当一个文本中存在两种及以上的语言字符时无法正确解析。
对于 0x00~0x7F 的值，ANSI 使用一个字节，因而兼容 ASCII。对于 0x7F 之后的值，使用两个字节。

GBK 是对 GB2312 的扩展，而 GB2312 是 ANSI 的一个代码页。

---
# 关于 GBK 编码的 BUG

新建一个空的文本文件，用记事本打开（必须是 Windows 自带的记事本），只输入“联通”二字保存关闭(输入“1联通”也是联通显示的也是乱码)，再重新打开时将是乱码。

当 txt 文档中一切字符都在 C0 ≤ AA（第一个字节） ≤ DF，80 ≤ BB（第二个字节） ≤ BF 这个范围时，notepad 无法确认文档的格式，会“猜测”其为 UTF-8。而"联通"就是 C1 AA CD A8，刚好在上面的范围内。因为保存和读取使用的是不同的编码方式，所以不能正常显现。

记事本默认是以 ANSI 编码保存文本文档的，而正是这种编码存在的 bug 招致了上述怪现象。假如保存时选择 Unicode、Unicode (Big Endian)、UTF-8 编码，就正常了。此外，假如以 ANSI 编码保存含有某些特别符号的文本文档，再次打开后符号也会变成英文问号。

# Unicode 速查

## 0x00-0xFF

|  - |0|1|2|3|4|5|6|7|8|9|A|B|C|D|E|F|
|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|
|0000| | | | | | | | | | | | | | | | |
|0010| | | | | | | | | | | | | | | | |
|0020| |!|"|#|$|%|&|'|(|)|*|+|,|-|.|/|
|0030|0|1|2|3|4|5|6|7|8|9|:|;|<|=|>|?|
|0040|@|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|
|0050|P|Q|R|S|T|U|V|W|X|Y|Z|[|\|]|^|_|
|0060|`|a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|
|0070|p|q|r|s|t|u|v|w|x|y|z|{|||}|~| |
|0080|€| |‚|ƒ|„|…|†|‡|ˆ|‰|Š|‹|Œ| |Ž| |
|0090| |‘|’|“|”|•|–|—|˜|™|š|›|œ| |ž|Ÿ|
|00A0| |¡|¢|£|¤|¥|¦|§|¨|©|ª|«|¬| |®|¯|
|00B0|°|±|²|³|´|µ|¶|·|¸|¹|º|»|¼|½|¾|¿|
|00C0|À|Á|Â|Ã|Ä|Å|Æ|Ç|È|É|Ê|Ë|Ì|Í|Î|Ï|
|00D0|Ð|Ñ|Ò|Ó|Ô|Õ|Ö|×|Ø|Ù|Ú|Û|Ü|Ý|Þ|ß|
|00E0|à|á|â|ã|ä|å|æ|ç|è|é|ê|ë|ì|í|î|ï|
|00F0|ð|ñ|ò|ó|ô|õ|ö|÷|ø|ù|ú|û|ü|ý|þ|ÿ|


---
# Java Web 涉及到的编码

摘自 [developerWorks-深入分析 Java 中的中文编码问题](https://www.ibm.com/developerworks/cn/java/j-lo-chinesecoding/)

对于使用中文来说，有 I/O 的地方就会涉及到编码，前面已经提到了 I/O 操作会引起编码，而大部分 I/O 引起的乱码都是网络 I/O，因为现在几乎所有的应用程序都涉及到网络操作，而数据经过网络传输都是以字节为单位的，所以所有的数据都必须能够被序列化为字节。在 Java 中数据被序列化必须继承 Serializable 接口。

这里有一个问题，你是否认真考虑过一段文本它的实际大小应该怎么计算，我曾经碰到过一个问题：就是要想办法压缩 Cookie 大小，减少网络传输量，当时有选择不同的压缩算法，发现压缩后字符数是减少了，但是并没有减少字节数。所谓的压缩只是将多个单字节字符通过编码转变成一个多字节字符。减少的是 String.length()，而并没有减少最终的字节数。例如将“ab”两个字符通过某种编码转变成一个奇怪的字符，虽然字符数从两个变成一个，但是如果采用 UTF-8 编码这个奇怪的字符最后经过编码可能又会变成三个或更多的字节。同样的道理比如整型数字 1234567 如果当成字符来存储，采用 UTF-8 来编码占用 7 个 byte，采用 UTF-16 编码将会占用 14 个 byte，但是把它当成 int 型数字来存储只需要 4 个 byte 来存储。所以看一段文本的大小，看字符本身的长度是没有意义的，即使是一样的字符采用不同的编码最终存储的大小也会不同，所以从字符到字节一定要看编码类型。

另外一个问题，你是否考虑过，当我们在电脑中某个文本编辑器里输入某个汉字时，它到底是怎么表示的？我们知道，计算机里所有的信息都是以 01 表示的，那么一个汉字，它到底是多少个 0 和 1 呢？我们能够看到的汉字都是以字符形式出现的，例如在 Java 中“淘宝”两个字符，它在计算机中的数值 10 进制是 28120 和 23453，16 进制是 6bd8 和 5d9d，也就是这两个字符是由这两个数字唯一表示的。Java 中一个 char 是 16 个 bit 相当于两个字节，所以两个汉字用 char 表示在内存中占用相当于四个字节的空间。

这两个问题搞清楚后，我们看一下 Java Web 中那些地方可能会存在编码转换？

用户从浏览器端发起一个 HTTP 请求，需要存在编码的地方是 URL、Cookie、Parameter。服务器端接受到 HTTP 请求后要解析 HTTP 协议，其中 URI、Cookie 和 POST 表单参数需要解码，服务器端可能还需要读取数据库中的数据，本地或网络中其它地方的文本文件，这些数据都可能存在编码问题，当 Servlet 处理完所有请求的数据后，需要将这些数据再编码通过 Socket 发送到用户请求的浏览器里，再经过浏览器解码成为文本。这些过程如下图所示：

图 3. 一次 HTTP 请求的编码示例
![一次 HTTP 请求的编码示例](https://www.ibm.com/developerworks/cn/java/j-lo-chinesecoding/image021-lg.png)

如上图所示一次 HTTP 请求设计到很多地方需要编解码，它们编解码的规则是什么？下面将会重点阐述一下：

## URL 的编解码

用户提交一个 URL，这个 URL 中可能存在中文，因此需要编码，如何对这个 URL 进行编码？根据什么规则来编码？有如何来解码？如下图一个 URL：

图 4.URL 的几个组成部分
![URL 的几个组成部分](https://www.ibm.com/developerworks/cn/java/j-lo-chinesecoding/image023.gif)

上图中以 Tomcat 作为 Servlet Engine 为例，它们分别对应到下面这些配置文件中：
Port 对应在 Tomcat 的 `<Connector port="8080"/>` 中配置，而 Context Path 在 `<Context path="/examples"/>` 中配置，Servlet Path 在 Web 应用的 web.xml 中的 `<url-pattern>` 中配置，PathInfo 是我们请求的具体的 Servlet，QueryString 是要传递的参数，注意这里是在浏览器里直接输入 URL 所以是通过 Get 方法请求的，如果是 POST 方法请求的话，QueryString 将通过表单方式提交到服务器端，这个将在后面再介绍。
```
<servlet-mapping> 
       <servlet-name>junshanExample</servlet-name> 
       <url-pattern>/servlets/servlet/*</url-pattern> 
</servlet-mapping>
```

上图中 PathInfo 和 QueryString 出现了中文，当我们在浏览器中直接输入这个 URL 时，在浏览器端和服务端会如何编码和解析这个 URL 呢？为了验证浏览器是怎么编码 URL 的我们选择 FireFox 浏览器并通过 HTTPFox 插件观察我们请求的 URL 的实际的内容，以下是 URL：`HTTP://localhost:8080/examples/servlets/servlet/ 君山 ?author= 君山` 在中文 FireFox3.6.12 的测试结果

图 5. HTTPFox 的测试结果
![HTTPFox 的测试结果](https://www.ibm.com/developerworks/cn/java/j-lo-chinesecoding/image025.jpg)

君山的编码结果分别是：e5 90 9b e5 b1 b1，be fd c9 bd，查阅上一届的编码可知，PathInfo 是 UTF-8 编码而 QueryString 是经过 GBK 编码，至于为什么会有“%”？查阅 URL 的编码规范 RFC3986 可知浏览器编码 URL 是将非 ASCII 字符按照某种编码格式编码成 16 进制数字然后将每个 16 进制表示的字节前加上“%”，所以最终的 URL 就成了上图的格式了。

默认情况下中文 IE 最终的编码结果也是一样的，不过 IE 浏览器可以修改 URL 的编码格式在选项 -> 高级 -> 国际里面的发送 UTF-8 URL 选项可以取消。

从上面测试结果可知浏览器对 PathInfo 和 QueryString 的编码是不一样的，不同浏览器对 PathInfo 也可能不一样，这就对服务器的解码造成很大的困难，下面我们以 Tomcat 为例看一下，Tomcat 接受到这个 URL 是如何解码的。

解析请求的 URL 是在 org.apache.coyote.HTTP11.InternalInputBuffer 的 parseRequestLine 方法中，这个方法把传过来的 URL 的 byte[] 设置到 org.apache.coyote.Request 的相应的属性中。这里的 URL 仍然是 byte 格式，转成 char 是在 org.apache.catalina.connector.CoyoteAdapter 的 convertURI 方法中完成的：
```
protected void convertURI(MessageBytes uri, Request request) 
throws Exception { 
       ByteChunk bc = uri.getByteChunk(); 
       int length = bc.getLength(); 
       CharChunk cc = uri.getCharChunk(); 
       cc.allocate(length, -1); 
       String enc = connector.getURIEncoding(); 
       if (enc != null) { 
           B2CConverter conv = request.getURIConverter(); 
           try { 
               if (conv == null) { 
                   conv = new B2CConverter(enc); 
                   request.setURIConverter(conv); 
               } 
           } catch (IOException e) {...} 
           if (conv != null) { 
               try { 
                   conv.convert(bc, cc, cc.getBuffer().length - 
cc.getEnd()); 
                   uri.setChars(cc.getBuffer(), cc.getStart(), 
cc.getLength()); 
                   return; 
               } catch (IOException e) {...} 
           } 
       } 
       // Default encoding: fast conversion 
       byte[] bbuf = bc.getBuffer(); 
       char[] cbuf = cc.getBuffer(); 
       int start = bc.getStart(); 
       for (int i = 0; i < length; i++) { 
           cbuf[i] = (char) (bbuf[i + start] & 0xff); 
       } 
       uri.setChars(cbuf, 0, length); 
}
```
从上面的代码中可以知道对 URL 的 URI 部分进行解码的字符集是在 connector 的 `<Connector URIEncoding=”UTF-8”/>` 中定义的，如果没有定义，那么将以默认编码 ISO-8859-1 解析。所以如果有中文 URL 时最好把 URIEncoding 设置成 UTF-8 编码。

QueryString 又如何解析？ GET 方式 HTTP 请求的 QueryString 与 POST 方式 HTTP 请求的表单参数都是作为 Parameters 保存，都是通过 request.getParameter 获取参数值。对它们的解码是在 request.getParameter 方法第一次被调用时进行的。request.getParameter 方法被调用时将会调用 org.apache.catalina.connector.Request 的 parseParameters 方法。这个方法将会对 GET 和 POST 方式传递的参数进行解码，但是它们的解码字符集有可能不一样。

POST 表单的解码将在后面介绍，QueryString 的解码字符集是在哪定义的呢？它本身是通过 HTTP 的 Header 传到服务端的，并且也在 URL 中，是否和 URI 的解码字符集一样呢？

从前面浏览器对 PathInfo 和 QueryString 的编码采取不同的编码格式不同可以猜测到解码字符集肯定也不会是一致的。的确是这样 QueryString 的解码字符集要么是 Header 中 ContentType 中定义的 Charset 要么就是默认的 ISO-8859-1，要使用 ContentType 中定义的编码就要设置 connector 的 <Connector URIEncoding=”UTF-8” useBodyEncodingForURI=”true”/> 中的 useBodyEncodingForURI 设置为 true。这个配置项的名字有点让人产生混淆，它并不是对整个 URI 都采用 BodyEncoding 进行解码而仅仅是对 QueryString 使用 BodyEncoding 解码，这一点还要特别注意。

从上面的 URL 编码和解码过程来看，比较复杂，而且编码和解码并不是我们在应用程序中能完全控制的，所以在我们的应用程序中应该尽量避免在 URL 中使用非 ASCII 字符，不然很可能会碰到乱码问题，当然在我们的服务器端最好设置 <Connector/> 中的 URIEncoding 和 useBodyEncodingForURI 两个参数。

## HTTP Header 的编解码

当客户端发起一个 HTTP 请求除了上面的 URL 外还可能会在 Header 中传递其它参数如 Cookie、redirectPath 等，这些用户设置的值很可能也会存在编码问题，Tomcat 对它们又是怎么解码的呢？

对 Header 中的项进行解码也是在调用 request.getHeader 是进行的，如果请求的 Header 项没有解码则调用 MessageBytes 的 toString 方法，这个方法将从 byte 到 char 的转化使用的默认编码也是 ISO-8859-1，而我们也不能设置 Header 的其它解码格式，所以如果你设置 Header 中有非 ASCII 字符解码肯定会有乱码。

我们在添加 Header 时也是同样的道理，不要在 Header 中传递非 ASCII 字符，如果一定要传递的话，我们可以先将这些字符用 org.apache.catalina.util.URLEncoder 编码然后再添加到 Header 中，这样在浏览器到服务器的传递过程中就不会丢失信息了，如果我们要访问这些项时再按照相应的字符集解码就好了。

## POST 表单的编解码

在前面提到了 POST 表单提交的参数的解码是在第一次调用 request.getParameter 发生的，POST 表单参数传递方式与 QueryString 不同，它是通过 HTTP 的 BODY 传递到服务端的。当我们在页面上点击 submit 按钮时浏览器首先将根据 ContentType 的 Charset 编码格式对表单填的参数进行编码然后提交到服务器端，在服务器端同样也是用 ContentType 中字符集进行解码。所以通过 POST 表单提交的参数一般不会出现问题，而且这个字符集编码是我们自己设置的，可以通过 request.setCharacterEncoding(charset) 来设置。

另外针对 multipart/form-data 类型的参数，也就是上传的文件编码同样也是使用 ContentType 定义的字符集编码，值得注意的地方是上传文件是用字节流的方式传输到服务器的本地临时目录，这个过程并没有涉及到字符编码，而真正编码是在将文件内容添加到 parameters 中，如果用这个编码不能编码时将会用默认编码 ISO-8859-1 来编码。

## HTTP BODY 的编解码

当用户请求的资源已经成功获取后，这些内容将通过 Response 返回给客户端浏览器，这个过程先要经过编码再到浏览器进行解码。这个过程的编解码字符集可以通过 response.setCharacterEncoding 来设置，它将会覆盖 request.getCharacterEncoding 的值，并且通过 Header 的 Content-Type 返回客户端，浏览器接受到返回的 socket 流时将通过 Content-Type 的 charset 来解码，如果返回的 HTTP Header 中 Content-Type 没有设置 charset，那么浏览器将根据 Html 的 <meta HTTP-equiv="Content-Type" content="text/html; charset=GBK" /> 中的 charset 来解码。如果也没有定义的话，那么浏览器将使用默认的编码来解码。

## 其它需要编码的地方

除了 URL 和参数编码问题外，在服务端还有很多地方可能存在编码，如可能需要读取 xml、velocity 模版引擎、JSP 或者从数据库读取数据等。
xml 文件可以通过设置头来制定编码格式
```
<?xml version="1.0" encoding="UTF-8"?>
```
Velocity 模版设置编码格式：
````
services.VelocityService.input.encoding=UTF-8
```
JSP 设置编码格式：
```
<%@page contentType="text/html; charset=UTF-8"%>
```
访问数据库都是通过客户端 JDBC 驱动来完成，用 JDBC 来存取数据要和数据的内置编码保持一致，可以通过设置 JDBC URL 来制定如 MySQL：url="jdbc:mysql://localhost:3306/DB?useUnicode=true&characterEncoding=GBK"。 
